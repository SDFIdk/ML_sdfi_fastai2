#training and validating on validations set after datacleaning

[MODEL]
#In order for the report to say what model the report is based on we need a path to a model
model_used_for_inference= 		../logs_and_models/example_dataset_iter_1/iter_1/models/iter_1.pth
ignore_index=0


[DATASET]
#The report need acess to the images that the model was evaluated on
dataset_folder= 		../multi_channel_dataset_creation/example_dataset
path_to_labels = 		../multi_channel_dataset_creation/example_dataset/labels/splitted_labels
path_to_all_txt=		../multi_channel_dataset_creation/example_dataset/data/all.txt
path_to_codes=			../multi_channel_dataset_creation/example_dataset/labels/codes.txt

#in order for the report to print out what dataset the model was trained on we need to insert a path to the dataset below
dataset_model_was_trained_on = ../multi_channel_dataset_creation/example_dataset

[CONFIG]
label_image_type= ".tif"

[SUBSETS]

all=				../multi_channel_dataset_creation/example_dataset/data/all.txt
valid=				../multi_channel_dataset_creation/example_dataset/data/valid.txt
train=				../multi_channel_dataset_creation/example_dataset/data/train.txt

[NAME]
#the report needs a title
job_name=			"iter_1_example_dataset" 

[FOLDERS]
#The csv file with statistics for each epoch is needed in order to plot graphs
log_file = 			../logs_and_models/example_dataset_iter_1/iter_1/logs/iter_1.csv
#The folder where infer.py saved all prediction-images
prediction_folder = 		../logs_and_models/example_dataset_iter_1/iter_1/models/example_dataset
#filename, error rate ,iou and confusion matrix is saved for each image in the evaluation dataset in the following file.  
model_performance_on_each_image_csv = ../logs_and_models/example_dataset_iter_1/iter_1/logs/model_performance_on_each_image_example_dataset.csv
#The filenames in  model_performance_on_each_image_csv  are relative to the following path
path_to_images = ../multi_channel_dataset_creation/example_dataset/data/splitted/rgb
#The report places temporary files here. 
report_cache_directory = ../logs_and_models/example_dataset_iter_1/iter_1/report_cache_example_dataset



[things_to_show]
# Normally you infer on the test set, so can show the fixed images
show_fixed_images = false
#Statistical meassures 
#show the error rate of the first subset on the first report page(forretningsraporte only has one subset == all.txt)
show_error_rate = true
show_statistics = true
#Show intro about performance and dataset
show_intro = false
show_plots = true
things_to_plot = ["valid_accuracy"]
#how many images of each kind that should be shown. if you cahnge this and create a new report it is good to delete the 'report_cache_directory' first in order to avoid confusion with old cached files
nr_of_images_to_show = 1

[CLASSMAPPINGS]
#If we only are interested in a subset of the classes that exists in the dataset we can remapp those classes to another class e.g background
#The remapping will also be performed on the predictions 
#To treat labels and predictions of class 6 and 7, as backgroubnd(0) use the following mapping: 
# class_remappings = {"6": 0, "7": 0}
#To not use any remapping, set it to {}
class_remappings = {}

[CASHING]
#Keep these variables at true in order to be sure that you not are visualizing old data saved in 'report_cache_directory'

#if we allready have done the 'model_performance_on_each_image_csv 'csv file you can set this variable to false
make_csv= true

#if we allready have created all images that visualize what pixels got misclassified you can set this variable to false. 

create_subset_files= true
